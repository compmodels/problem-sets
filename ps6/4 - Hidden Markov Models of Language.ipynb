{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# special import for running the viterbi algorithm\n",
    "from hmm import viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class='alert alert-success'>In this problem, we will use a hidden Markov model (HMM) as a model of human language. You can consult AIMA3, pages 578-581 and 892-896, or AIMA2, pages 549-551, 834-840 for help.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "![](images/hmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Recall that in an HMM a sequence of $T$ observations $(w_1,w_2,\\ldots,w_T)$ is assumed to be generated by an underlying sequence of $T$ hidden (i.e., unobserved) *state variables*, $(s_1,s_2,\\ldots,s_T)$. The *Markov* in the model name comes from the assumption that the next state variable, $s_{t+1}$, depends only on the current state, $s_{t}$. In this way, the state variables $(s_1,s_2,\\ldots,s_T)$ form a **Markov chain**. It is assumed that each observation variable $w_t$ depends only on the corresponding state variable, $s_t$ (this is reflected in the diagram above—at step $t$ an observation node/variable $w_t$ is connected to a single state node/variable $s_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Using the HMM as a model of human language, the variables $(w_1,w_2,\\ldots,w_T)$ correspond to the sequence of words in an observed sentence of length $T$, and the hidden state variables $(s_1,s_2,\\ldots,s_T)$ correspond to the (unobserved) parts of speech of the words in the sentence, such as *noun*, *verb*, and *adjective*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    " Recall that an HMM has three sets of parameters: (1) the initial state probability distribution $P(s_1)$, which specifies the probabilities with which the the first state takes on each of its possible values, (2) the transition probabilities $P(s_t | s_{t-1})$, which specify the probability of the next hidden state given the previous one, and (3) the emission probabilities $P(w_t | s_t)$, which specify the probability of the observation at time $t$ given on the hidden state at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "There are two primary applications of HMMs to language processing. The first is *state estimation*, where the model parameters are all known, and we are trying to recover the sequence of hidden states $(s_1,s_2,\\ldots,s_T)$ from the sequence of observations $(w_1,w_2,\\ldots,w_T)$. In our language modeling context, this would be like assuming we already have a good model of English grammar that specifies the HMM model parameters, and we want to take an observed English sentence and perform part-of-speech tagging to determine the part of speech (e.g., adjective, noun, or preposition) of each word in the sentence. The second application of HMMs is *parameter estimation*, where the model parameters are unknown, and we are trying to recover them from only a set of observation sequences of the form $(w_1,w_2,\\ldots,w_T)$. Because the hidden state variables are also unknown when performing parameter estimation, they must also be estimated simultaneously. In this situation, the expectation-maximization (EM) algorithm is used to perform simultaneous state estimation and parameter estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "### Part A (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "We will use a simplistic English grammar consisting of only three\n",
    "parts of speech (*noun*, *verb*, and *adjective*) and ten words (*john*, *sally*, *reddit*, *love*,\n",
    "*parks*, *dogs*, *exhausted*, *marbled*,\n",
    "*big*, and *inappropriate*). Some of the words can be used as multiple parts of speech, as demonstrated in the following table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td> Word </td><td> Noun </td><td> Verb </td><td> Adjective </td>\n",
    "</tr><tr>\n",
    "<td> john </td><td> x </td><td> </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> sally </td><td> x </td><td> </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> reddit </td><td> x </td><td> </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> love </td><td> x </td><td> x </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> parks </td><td> x </td><td> x </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> dogs </td><td> x </td><td> x </td><td> </td>\n",
    "</tr><tr>\n",
    "<td> exhausted </td><td> </td><td> x </td><td> x </td>\n",
    "</tr><tr>\n",
    "<td> marbled </td><td> </td><td> x </td><td> x </td>\n",
    "</tr><tr>\n",
    "<td> big </td><td> </td><td> </td><td> x </td>\n",
    "</tr><tr>\n",
    "<td> inappropriate </td><td> </td><td> </td><td> x </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Furthermore, we will assume that all sentences consist of exactly five\n",
    "words, so that $T = 5$ in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c01c9981dc2f693807ed95bdc152a0f5",
     "grade": false,
     "grade_id": "variable_definitions",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#possible values of the hidden state: S_t=j means the part of speach is partsOfSpeech[j]\n",
    "partsOfSpeech = ['noun','verb','adjective']\n",
    "\n",
    "#initProbs[j] is P(S_0=j)\n",
    "initProbs = np.array([ .59, .01, .4])\n",
    "\n",
    "#the possible emissions: W_t=j means the word is words[j]\n",
    "words = ['john','sally','reddit','love','parks','dogs',\n",
    "         'exhausted','marbled','big','inappropriate']\n",
    "\n",
    "#transitionProbs[i,j] is P(S_{t+1}=i| S_t=j)\n",
    "transitionProbs = np.array([[.02, .3, .69], \n",
    "                            [.97, .01, .01], \n",
    "                            [.01, .69, .3]])\n",
    "\n",
    "#emissionProbs[i,j] is p(W_t=i | S_t=j)\n",
    "emissionProbs = np.array([[1/6, 0, 0], [1/6, 0, 0], [1/6, 0, 0], \n",
    "                          [1/6, .2, 0], [1/6, .2, 0], [1/6, .2, 0],\n",
    "                          [0, .2, .25], [0, .2, .25], [0, 0, .25], \n",
    "                          [0, 0, .25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "First we will define several variables:\n",
    "\n",
    "`partsOfSpeech`: a list of the three possible values of the hidden state variables (corresponding to the three parts of speech). Each part of speech (*noun*, *verb*, and *adjective*) corresponds to a numerical index according to its position in this list.\n",
    "\n",
    "`words`: a list containing each of the ten words in our vocabulary. Each item in word is given a numerical index according to its position in this list.\n",
    "\n",
    "`initProbs`: A NumPy array containing the initial state probabilities $P(s_1)$, where the entry in row $i$ and column $j$ gives the probability $P(s_t = i | s_{t-1} = j)$ of transitioning from state $j$ to state $i$.\n",
    "\n",
    "`emissionProbs`: A Numpy array containing the emission probabilities, where the entry in row $i$ and column $j$ gives the probability $P(w_t = i | s_t = j)$ of observing word $i$ when in state $j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "To produce a sentence we will choose a part of speech (POS) according to the initial probabilities. After choosing a POS, we choose a corresponding word given the emission probababilities for that POS. Thereafter we transition to the next POS according to the transition matrix, and choose another word accoring to the emission probabilities. This process of transitioning to a new hidden state (POS) and emitting a word repeats until we complete a five word sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function generateSentence so that it generates a random sentence (each sentence contains 5 words) following the probabilities in the grammar. You may wish to use `np.random.choice` to choose among a set of options weighted by a vector of probabilities.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a9226b1bf47f7b89c980cac3b8a14e36",
     "grade_id": "generateSentence",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generateSentence(initProbs, words, transitionProbs, emissionProbs, partsOfSpeech):        \n",
    "    \"\"\"\n",
    "    Constructs a sentence according to the probabilities in the HMM model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initProbs: 1*n array\n",
    "        initProbs encodes the probability of each of n hidden states\n",
    "        \n",
    "    words : list of length 10\n",
    "        A list containing each of the ten words in our vocabulary. \n",
    "\n",
    "    transitionProbs: n*n array\n",
    "        transitionProbs encodes the probability of transitioning from\n",
    "        any of n hidden states to any other state\n",
    "    \n",
    "    emissionProbs: m*n array\n",
    "        emissionProbs encodes the probability of emitting a particular \n",
    "        word given the current hidden state, where m is the number of words\n",
    "        and n is the number of hidden states\n",
    "    \n",
    "    partsOfSpeech: 1*n array\n",
    "        partsOfSpeech contains the names of the parts of speech that\n",
    "        correspond to the indices for initProbs, transitionProbs, etc.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    a dict with two keys, pos and sentence\n",
    "    pos: the parts of speech of each word in the sentence\n",
    "    sentence: the words in the sentence, as a list\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "generateSentence(initProbs, words, transitionProbs, emissionProbs, partsOfSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "88b3c2734b3266c27b992a930508086e",
     "grade": true,
     "grade_id": "test_generateSentence",
     "points": 1
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# basic length and type checking\n",
    "test = generateSentence(initProbs, words, transitionProbs, emissionProbs, \\\n",
    "                        partsOfSpeech)\n",
    "assert_equal(len(test['pos']),5)\n",
    "assert_equal(len(test['sentence']),5)\n",
    "assert(all([isinstance(x, str) for x in test['pos']]))\n",
    "assert(all([isinstance(x, str) for x in test['sentence']]))\n",
    "\n",
    "# check that every word has a nonzero emission probability for \n",
    "# the corresponding hidden state. Try 20 sentences\n",
    "for i in range(20):\n",
    "    test = generateSentence(initProbs, words, transitionProbs, emissionProbs, \\\n",
    "                        partsOfSpeech)\n",
    "    for j in range(5):\n",
    "        word = test['sentence'][j]\n",
    "        singlePos = test['pos'][j]  \n",
    "        assert(emissionProbs[words.index(word), \n",
    "                         partsOfSpeech.index(singlePos)] > 0) \n",
    "\n",
    "#Test the hidden-state sequence: Check that nouns are almost always followed by verbs. Try 100 sentences.\n",
    "nrNouns=0;\n",
    "nrNounVerbTransitions=0;\n",
    "for i in range(100):\n",
    "    test = generateSentence(initProbs, words, transitionProbs, emissionProbs, \\\n",
    "                        partsOfSpeech)\n",
    "    for j in range(4):\n",
    "        fromPos = test['pos'][j]  \n",
    "        toPos = test['pos'][j+1]\n",
    "        if fromPos=='noun':\n",
    "            nrNouns+=1\n",
    "            if toPos=='verb':\n",
    "                nrNounVerbTransitions+=1\n",
    "assert(abs(nrNounVerbTransitions/nrNouns - transitionProbs[1,0]) < 0.1)\n",
    "\n",
    "print(\"Success!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Judging from the randomly-sampled sentences, does the HMM seem to be a good model of our limited subset of the English language? Why or why not?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f7110f5f4c912ff8c2911f6c7039962",
     "grade": true,
     "grade_id": "hmmGoodModel",
     "points": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "### Part B (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "We will now tag parts of speech in our sentence by estimating the hidden state variables in our hidden Markov model. As described above, state estimation refers to estimating the sequence of hidden states of an HMM corresponding to a sequence of observations. This can only be performed when the model parameters (i.e., the initial hidden state probabilities, the transition probabilities, and the emission probabilities) are already known; otherwise, simultaneous _parameter estimation_ must also be done using the **EM algorithm**. \n",
    "\n",
    "For now, we will assume that all the parameters for our HMM are known, which makes the problem easier. We can perform state estimation in Python using the **Viterbi algorithm**. The Viterbi algorithm is a popular algorithm for HMMs that returns the sequence of hidden states which maximizes the total joint probability of all the hidden states and observations in the graphical model for the HMM.\n",
    "\n",
    "We have created a wrapper function `viterbi` which takes as inputs a sentence and the HMM model parameters described above and calls the Viterbi algorithm to return the most likely sequence of hidden state variables for the given sentence. `viterbi` returns a set of indices; use `partsOfSpeech` to interpret these indices. For example, if we represent the sentence \"love dogs exhausted big exhausted\" in terms of the corresponding indices in `words`, `[3, 5, 6, 8, 6]`, we can call `viterbi` to get the indices of the most likely hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "viterbi([3, 5, 6, 8, 6], initProbs, transitionProbs, emissionProbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function `part_of_speech_tagging` to produce the highest probability tags for a set of input sentences. Inside your function, call the `viterbi` fucntion to find the best set of tags (hiddens states) for each sentence. Make sure function returns the names of the parts of speech, *not* their numerical indices.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "400dfa6d7c01e643e8effa243ecf34ce",
     "grade_id": "part_of_speech_tagging",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def part_of_speech_tagging(sentences, words, initProbs, partsOfSpeech,\n",
    "                           transitionProbs, emissionProbs):\n",
    "    \"\"\"\n",
    "    Identifies the parts of speech for each of the words in a collection\n",
    "    of sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : numpy array of shape (n,)\n",
    "        An array of n lists of strings. Each sentence corresponds to\n",
    "        one list. The number of strings (words) in a sentence (list)\n",
    "        may vary.\n",
    "    \n",
    "    words : list of length 10\n",
    "        A list containing each of the ten words in our vocabulary. \n",
    "        \n",
    "    partsOfSpeech: list of length 3\n",
    "        An array containing the three possible values of the hidden \n",
    "        state variables (corresponding to the three parts of speech).\n",
    "\n",
    "    initProbs: numpy array of shape (3,)\n",
    "        An array containing the initial state probabilities P(s1), \n",
    "        where the entry in row i and column j gives the probability \n",
    "        P(st=i|st−1=j) of transitioning from state j to state i.\n",
    "        \n",
    "    emissionProbs: numpy array of shape (10,3)\n",
    "        A matrix containing the emission probabilities, where the \n",
    "        entry in row i and column j gives the probability P(wt=i|st=j)\n",
    "        of observing word i when in state j.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    speech_tags : numpy array of shape (n, T)\n",
    "        An array of lists, where each list contains the parts of speech\n",
    "        of the T words in the sentence.    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Once you have implemented the `part_of_speech_tagging` function, you can test it on the provided sentences with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "sentences = np.array([\n",
    "             ['exhausted', 'dogs', 'love', 'marbled', 'parks'],\n",
    "             ['inappropriate', 'sally', 'love', 'inappropriate', 'reddit'],\n",
    "             ['exhausted', 'exhausted', 'sally', 'parks', 'dogs'],\n",
    "             ['sally', 'dogs', 'big', 'exhausted', 'john'],\n",
    "             ['big', 'john', 'exhausted', 'exhausted', 'dogs'],\n",
    "            ])\n",
    "\n",
    "speech_tags = part_of_speech_tagging(sentences, words, initProbs, \n",
    "                                     partsOfSpeech, transitionProbs, \n",
    "                                     emissionProbs)\n",
    "\n",
    "for idx,s in enumerate(sentences):\n",
    "    print('sentence:    ' + str(s))\n",
    "    print('speech tags: ' + str(speech_tags[idx])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "40934c3a42d83b77f54d59d8787a1a72",
     "grade": true,
     "grade_id": "test_part_of_speech_tagging",
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Is the part_of_speech_tagging function correctly implemented?\"\"\"\n",
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# create some new sentences for testing\n",
    "test_sentences = np.array([\n",
    "                    ['sally', 'reddit', 'john', 'big', 'parks'],\n",
    "                    ['reddit', 'parks', 'john', 'big', 'sally'],\n",
    "                    ['john', 'big', 'dogs', 'love', 'parks']\n",
    "                    ])\n",
    "\n",
    "test_tags = np.array([\n",
    "                ['noun', 'noun', 'noun', 'adjective', 'noun'],\n",
    "                ['noun', 'verb', 'noun', 'adjective', 'noun'],\n",
    "                ['noun', 'adjective', 'noun', 'verb', 'noun'],\n",
    "                ])\n",
    "\n",
    "p_o_s = part_of_speech_tagging(test_sentences, words, initProbs, partsOfSpeech, \n",
    "                               transitionProbs, emissionProbs)\n",
    "\n",
    "# is the output of the correct shape?\n",
    "assert_equal(p_o_s.shape, test_tags.shape)\n",
    "\n",
    "# is the output an array of strings?\n",
    "assert(all([isinstance(x, str) for x in p_o_s.flatten()]))\n",
    "\n",
    "# are the correct speech tags being returned?\n",
    "for idx, t in enumerate(test_tags):\n",
    "    assert_array_equal(p_o_s[idx], t, \"Incorrect speech tags generated for one of test_sentences\")\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "\n",
    "### Part C (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Does the HMM model do a good job of recovering the parts of speech of the words in our limited subset of the English language? Which words in the above sentences are used as more than one part of speech? (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "be491152523ca947d5881b1828430d54",
     "grade": true,
     "grade_id": "hmmAnalysis1",
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Was the HMM able to determine the correct part of speech for each occurrence of these words? Give a short explanation of why the HMM model was or was not able to accomplish this disambiguation task. (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d54813bda252481785b5023f493282ef",
     "grade": true,
     "grade_id": "hmmAnalysis2",
     "locked": false,
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
