{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Recall from lecture and readings that Tversky defined the similarity between objects $a$ and $b$ as:\n",
    "\n",
    "$$\n",
    "S(a,b) = \\theta\\,f(A \\cap B) - \\alpha\\,f(A - B) - \\beta\\,f(B-A)\n",
    "$$\n",
    "\n",
    "Here, $A$ is the set of features of $a$, $B$ is the set of features of $b$, $f$ is an additive function from sets to numbers, and $\\theta, \\alpha, \\beta$ are free parameters all $\\ge 0$.\n",
    "\n",
    "In this problem you will write code to implement Tversky's contrast model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part A (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We will compute each part of the similarity function in turn. First, we need to compute $f(A\\cap B)$, which is essentially the number of features that $A$ and $B$ have in common. As an example, consider the following fruits:\n",
    "\n",
    "|            | Sweet | Sour  | Bitter | Salty | Seeds |\n",
    "|:-----------|:-----:|:-----:|:------:|:-----:|:-----:|\n",
    "| Orange     | 1     | 1     | 0      | 0     | 1     |\n",
    "| Lemon      | 0     | 1     | 1      | 0     | 1     |\n",
    "\n",
    "As NumPy arrays, these would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "orange_features = np.array([True,  True,  False, False, True ])\n",
    "lemon_features  = np.array([False, True,  True,  False, True ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "What are the number of features in common between the orange and lemon? A feature only counts as being \"in common\" if *both* feature vectors have it, so in this case, the only common features between orange and lemon are \"sour\" and \"seeds\" (not \"salty\", because neither of them have this feature). Thus, orange and lemon have two features in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function `common_features` so that it takes two binary feature vectors of length $n$ (`a` and `b`) as arguments and returns the total number of features in common between `a` and `b`. Here, we define the \"number of common features\" as the number of locations that are `True` in both `a` and `b`.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "da924ec15b466f84257b93c5364bd642",
     "grade": false,
     "grade_id": "common_features",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def common_features(a, b):\n",
    "    \"\"\"\n",
    "    Compute the number of common features between a and b. Features \n",
    "    count as being shared between the vectors if they are present in\n",
    "    both vectors (i.e., they are a 1 in both). In other words, you should\n",
    "    compute the intersection of features between a and b.\n",
    "    \n",
    "    Hint: your solution can be done in a single line of code, including\n",
    "    the return statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b : boolean numpy array with shape (n,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    number of common features between a and b\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Test our your `common_features` function on the orange and lemon features, to see if it does in fact return 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "common_features(orange_features, lemon_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "22c298084eaa9c4b718fd285ba5f3616",
     "grade": true,
     "grade_id": "test_common_features",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that common_features is correct.\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "assert_equal(common_features(np.array([True, True, False, False]), np.array([True, True, False, False])), 2)\n",
    "assert_equal(common_features(np.array([True, False, False, False]), np.array([True, True, False, False])), 1)\n",
    "assert_equal(common_features(np.array([True, False, True, False]), np.array([True, True, False, False])), 1)\n",
    "assert_equal(common_features(np.array([False, False, False, False]), np.array([False, False, False, False])), 0)\n",
    "assert_equal(common_features(np.array([True, True, True, True]), np.array([True, True, True, True])), 4)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part B (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "In the next two terms of the equation, we need to compute $f(A-B)$ and $f(B-A)$. This can be done using the same operation: computing the number of features that are in one vector, but not the other. As an example, let's take a look at some more fruits:\n",
    "\n",
    "|            | Sweet | Sour  | Bitter | Salty | Seeds |\n",
    "|:-----------|:-----:|:-----:|:------:|:-----:|:-----:|\n",
    "| Grapefruit | 1     | 1     | 1      | 0     | 1     |\n",
    "| Banana     | 1     | 0     | 0      | 0     | 0     |\n",
    "\n",
    "If we wanted to compute $f(\\textbf{grapefruit}-\\textbf{banana})$, we want to see what features grapefruit has that banana does not. In this case, there are three features matching this description: \"sour\", \"bitter\", and \"seeds\". Similarly, to compute $f(\\textbf{banana}-\\textbf{grapefruit})$, we want to look at features that are in banana but not in grapefruit. Here, there are actually *no* features that the banana has that the grapefruit does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function `differences` so that it takes two binary feature vectors of length $n$ (`a` and `b`) as arguments and returns the total number of features in `a` that are not contained in `b`. This is defined as the number of locations that are `True` in `a` and `False` in `b`.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f22271ed780158a4f6c8ca4dd6d2986",
     "grade": false,
     "grade_id": "differences",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def differences(a, b):\n",
    "    \"\"\"\n",
    "    Compute the number of features that belong to a, but not b. Features \n",
    "    count as being in a but not b if the feature is 1 in a, and 0 in b.\n",
    "    \n",
    "    Hint: your solution can be done in a single line of code, including\n",
    "    the return statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b : boolean numpy array with shape (n,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    number of differences between a and b\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Test your `differences` function on the orange and lemon feature vectors to see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# define the feature vectors\n",
    "grapefruit_features = np.array([True,  True,  True,  False, True ])\n",
    "banana_features     = np.array([True,  False, False, False, False])\n",
    "\n",
    "print(\"f(grapefruit - banana) = \" + str(differences(grapefruit_features, banana_features)))\n",
    "print(\"f(banana - grapefruit) = \" + str(differences(banana_features, grapefruit_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "31dba14330b9b48e36a322cf3fa97bf1",
     "grade": true,
     "grade_id": "test_differences",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that differences is correct.\"\"\"\n",
    "assert_equal(differences(np.array([True, True, False, False]), np.array([True, True, False, False])), 0)\n",
    "assert_equal(differences(np.array([True, False, False, False]), np.array([True, True, False, False])), 0)\n",
    "assert_equal(differences(np.array([True, False, True, False]), np.array([True, True, False, False])), 1)\n",
    "assert_equal(differences(np.array([True, True, True, True]), np.array([False, False, False, False])), 4)\n",
    "assert_equal(differences(np.array([True, True, True, True]), np.array([False, False, False, True])), 3)\n",
    "assert_equal(differences(np.array([False, False, False, False]), np.array([False, False, False, False])), 0)\n",
    "assert_equal(differences(np.array([True, True, True, True]), np.array([True, True, True, True])), 0)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part C (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Now, using your completed functions `common_features` and `differences`, compute Tversky's similarity function in `tversky_sim` below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c9ac47fab00c0c69e5d718535ca1acc2",
     "grade": false,
     "grade_id": "tversky_sim",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tversky_sim(a, b, theta=1.0, alpha=0.5, beta=0.5):\n",
    "    \"\"\"\n",
    "    Compute Tversky's similarity function for two vectors a and b:\n",
    "    \n",
    "    S(a, b) = theta*f(a ∩ b) - alpha*f(a - b) - beta*f(b - a)\n",
    "    \n",
    "    Hint: your solution can be done in 4 lines of code (or less), including\n",
    "    the return statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b : boolean numpy array with shape (n,)\n",
    "    theta, alpha, beta : parameters of the similarity function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the similarity between a and b\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "812829d3c0f655de76c20f5323652727",
     "grade": true,
     "grade_id": "test_tversky_sim",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([True, True, True, False, False, False])\n",
    "y = np.array([False, True, True, True, False, True])\n",
    "\n",
    "# check some explicit values\n",
    "assert_equal(tversky_sim(x, y), 0.5)\n",
    "assert_equal(tversky_sim(y, x), 0.5)\n",
    "assert_equal(tversky_sim(x, y, theta=2.0), 2.5)\n",
    "assert_equal(tversky_sim(y, x, theta=2.0), 2.5)\n",
    "assert_equal(tversky_sim(x, y, alpha=1.0), 0.0)\n",
    "assert_equal(tversky_sim(y, x, alpha=1.0), -0.5)\n",
    "assert_equal(tversky_sim(x, y, beta=1.5), -1.5)\n",
    "assert_equal(tversky_sim(y, x, beta=1.5), -0.5)\n",
    "\n",
    "# check that it uses common_featues\n",
    "old_common_features = common_features\n",
    "del common_features\n",
    "try:\n",
    "    tversky_sim(x, y)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"tversky_sim does not use common_features\")\n",
    "finally:\n",
    "    common_features = old_common_features\n",
    "    del old_common_features\n",
    "\n",
    "# check that it uses differences\n",
    "old_differences = differences\n",
    "del differences\n",
    "try:\n",
    "    tversky_sim(x, y)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"tversky_sim does not use differences\")\n",
    "finally:\n",
    "    differences = old_differences\n",
    "    del old_differences\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part D (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Now that we have a way of quantifying similarity, let's apply it to some real data. Here we have provided you with a dataset that includes 50 animals and 80 features, and specifies which animals have which features.\n",
    "\n",
    "First, let's load our data in. There are three arrays in the data: `feature_names`, `animal_names`, and `animal_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data/50animals.npz\")\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The `animal_features` array corresponds to a $50\\times 80$ boolean array of features, where each row corresponds to a different animal, and each column corresponds to a different feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "animal_features = data['animal_features']\n",
    "animal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "And `animal_names` corresponds to a vector of length 50 of the animal names. For convenience, we're going to convert this to a list (and only show the first 10 animal names, since the list itself is quite long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "animal_names = list(data['animal_names'])\n",
    "animal_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Similarly, the `feature_names` array is a vector of length 85 of the feature names. We actually won't need it for this problem, though, so we won't create a new variable for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function `find_similar_animals` to take the name of an animal and find the **5 most similar animals** to that animal, using your function `tversky_sim`. You should return the animals in order of most similar to least similar, and you should *not* return the name of the animal that was passed in.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Note: the `np.argsort()` function will come in handy here (take a look at Problem Set 0 if you forget how it's used). To keep ties in the original order, make sure to use mergesort (which is [stable](http://programmers.stackexchange.com/a/247441)) as so:\n",
    "\n",
    "```\n",
    "indices = np.argsort(array, kind='mergesort')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c81d9bad0684cd708eceb7617481b00b",
     "grade": false,
     "grade_id": "find_similar_animals",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_animals(name, features, animal_names):\n",
    "    \"\"\"\n",
    "    Finds the five most similar animals to the given animal. You should return the\n",
    "    animals in order from most similar to least similar to the given animal. In\n",
    "    addition, you should NOT include the given animal in the list of animals you\n",
    "    return. \n",
    "    \n",
    "    If two animals have the same similarity score, find_similar_animals \n",
    "    should break ties in the REVERSE of the order they appear in animal_names \n",
    "    (e.g., if the first two entries in animal_names are A and B, and both animals \n",
    "    A and B have the same similarity to target animal C, find_similar_animals should \n",
    "    place B BEFORE A when ranking them in terms of their similarity to C.)\n",
    "    \n",
    "    Hint: your solution can be done in 4 lines of code, including the return\n",
    "    statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        the name of an animal\n",
    "    features : boolean numpy array\n",
    "        animals by features, with shape (n, m)\n",
    "    animal_names : list of strings\n",
    "        list of animal names with length n\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of five animal names\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Use your function to find out what animals are most similar to a mouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# should print ['rat', 'rabbit', 'weasel', 'hamster', 'squirrel']\n",
    "find_similar_animals('mouse', animal_features, animal_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5f4c2b41df4dbcd1bff5f3b8cc02eed",
     "grade": true,
     "grade_id": "test_find_similar_animals",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that find_similar_animals is correct\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "def assert_one_equal(arr, *others):\n",
    "    for other in others:\n",
    "        if arr == other:\n",
    "            return\n",
    "    assert_equal(arr, others[0])\n",
    "\n",
    "# load the animal data\n",
    "data = np.load(\"data/50animals.npz\")\n",
    "af = data['animal_features']\n",
    "an = list(data['animal_names'])\n",
    "data.close()\n",
    "\n",
    "# try finding animals similar to mouse\n",
    "assert_equal(\n",
    "    find_similar_animals('mouse', af, an),\n",
    "    [u'rat', u'rabbit', u'weasel', u'hamster', u'squirrel'])\n",
    "\n",
    "# try finding animals similar to grizzly bear\n",
    "assert_equal(\n",
    "    find_similar_animals('grizzly bear', af, an),\n",
    "    ['bobcat', 'polar bear', 'raccoon', 'lion', 'gorilla'])\n",
    "\n",
    "# try finding animals similar to grizzly bear with different features\n",
    "assert_equal(\n",
    "    find_similar_animals('grizzly bear', ~af, an),\n",
    "    ['polar bear', 'gorilla', 'german shepherd', 'bobcat', 'raccoon'])\n",
    "\n",
    "# try finding animals similar to grizzly bear with different names\n",
    "assert_equal(\n",
    "    find_similar_animals('grizzly bear', af, an[::-1]),\n",
    "    ['weasel', 'beaver', 'buffalo', 'tiger', 'collie'])\n",
    "\n",
    "# try finding animals similar to grizzly bear with both different names and features\n",
    "assert_equal(\n",
    "    find_similar_animals('grizzly bear', ~af, an[::-1]),\n",
    "    ['spider monkey', 'seal', 'weasel', 'dalmatian', 'giraffe'])\n",
    "\n",
    "# check that it uses tversky_sim\n",
    "old_tversky_sim = tversky_sim\n",
    "del tversky_sim\n",
    "try:\n",
    "    find_similar_animals('mouse', af, an)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"find_similar_animals does not use tversky_sim\")\n",
    "finally:\n",
    "    tversky_sim = old_tversky_sim\n",
    "    del old_tversky_sim\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part E (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Run your function `find_similar_animals` for the input 'giant panda':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "find_similar_animals('giant panda', animal_features, animal_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">What are the five most similar animals it returns to the panda? Do they match your intuitions (that is, if you were to intuitively pick out the five most similar animals to a panda, would you pick those five in that order)? (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b15b28b14cdcdf465beb81a96a0c77a6",
     "grade": true,
     "grade_id": "part_e",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\"> If yes, why do you think Tversky's contrast model does a good job at capturing your intuitions? If no, what aspect of Tversky's contrast model leads to the contradiction with your intuitions? (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e347daa05ec97253664a2c7bbbb5da9",
     "grade": true,
     "grade_id": "part_e2",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part F (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Tversky's contrast model takes optional parameters, $\\theta$, $\\alpha$, and $\\beta$, which bias the similarity more or less towards shared features versus feature differences. Recall from lecture that Tversky's notion of similarity says that the similarity of the *variant* to the *prototype* should be greater than the similarity of the *prototype* to the *variant*. More formally, if $a$ is the prototype and $b$ is the variant, then:\n",
    "\n",
    "$$\n",
    "S(b,a) − S(a,b) = (\\alpha −\\beta)[ f (A − B) − f (B − A)]\n",
    "$$\n",
    "\n",
    "Given this equation, $S(b,a)>S(a,b)$ when $\\alpha>\\beta$ *and* $f(A-B)>f(B-A)$ -- that is, when the prototype has more distinctive or heavily weighted features than the variant. Previously, we used the same value for both $\\alpha$ and $\\beta$, meaning that $S(b,a)==S(a,b)$. However, if we change these parameters, then we can get asymmetric similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Let's explore this idea a little further, in the case of the bat and the mouse. First, which animal you would intuitively say is more prototypical: the bat or the mouse? Why?  (**0.1 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2906267597fc7671a94df1d0e06238d5",
     "grade": true,
     "grade_id": "part_f_1",
     "locked": false,
     "points": 0.1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Now, run the cell below to find out what Tversky's similarity metric says about the similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9bad7b5d7d43f5bb8c2786f8ce6b710a",
     "grade": false,
     "grade_id": "bat_mouse_similarity",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "leopard = animal_features[animal_names.index('leopard')]\n",
    "tiger = animal_features[animal_names.index('tiger')]\n",
    "print(\"S(leopard, tiger) = {}\".format(tversky_sim(leopard, tiger, theta=1.0, alpha=2.5, beta=0.5)))\n",
    "print(\"S(tiger, leopard) = {}\".format(tversky_sim(tiger, leopard, theta=1.0, alpha=2.5, beta=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Does Tversky's similarity metric say that a leopard is more similar to a tiger, or vice versa? Does this mean that Tversky's similarity metric says (in this case) that the prototype is more similar to the variant, or that the variant is more similar to the prototype? (Hint: What animal category are leopards and tigers members of? Which do you think is a more \"prototypical\" example of this category?) (**0.4 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1c3f618574974f16ced790901b3d0da6",
     "grade": true,
     "grade_id": "part_f_2",
     "locked": false,
     "points": 0.4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Can Tversky's leopard vs. tiger similarity results be interpreted as counter-evidence for his notion that $S$(variant, prototype) $>$ $S$(prototype, variant)?  (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ed4a65f21cd85ecc286d115279338ff4",
     "grade": true,
     "grade_id": "part_f_3",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
